---
title: "JHU Practical Machine Learning Course Project"
author: "Michael Pannucci"
date: "2023-07-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

We have been tasked with using accelerometer data to predict the outcome variable *classe*, which is a rating of how well someone performed a weightlifting movement using sensors based on the belt, forearm, arm, and dumbbell of six study participants. We have been provided a training set of nearly 20,000 observations and a testing set of 20 observations. We will apply a few machine learning algorithms learned in this course to model the training data and apply the one we think fits best to the testing data.

## Preliminaries

We begin by activating the needed libraries and setting a seed number for reproducibility.

``` {r prelim, message = FALSE, results = "hide"}
library(caret)
library(rattle)
set.seed(467)
```

## Loading the Data

Next, we will load in the data as linked from the course website.

``` {r load_data}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainURL))
testing <- read.csv(url(testURL))
```

## Cleaning the Data

We will apply three cleaning steps to get the data ready for modeling.

1. Remove the first column (*X*) since it is just a index variable.
2. Remove columns with too many NA values (we use 75% as the cutoff here).
3. Remove columns with near-zero variance using the handy **nearZeroVariance()** function.

``` {r clean_data}
training <- training[, colMeans(is.na(training)) < 0.75]
nzv <- nearZeroVar(training)
training <- training[, -c(1, nzv)]
```

## Training and Validation Sets

We will leave the provided testing set alone and use it later to, unsurprisingly, test our chosen model to predict the outcome of interest. Using the now-cleaned training data, let's split that into an active training set and a validation set.

``` {r training_validation}
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
mytrain <- training[inTrain, ]
myval <- training[-inTrain, ]
```

## Cross-Validation

As it is good practice to cross-validate our training models, let's declare this using the **trainControl()** function using three folds.

``` {r control}
control <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
```

## Training Models

### Model 1: Decision Tree

Our first model will be a decision tree. Let's fit the model, use it to predict values from our validation set, and then save the confusion matrix.

``` {r model1}
model1 <- train(classe ~ ., data = mytrain, method = "rpart", trControl = control)
fancyRpartPlot(model1$finalModel)
pred1 <- predict(model1, myval)
cfmatrix1 <- confusionMatrix(pred1, factor(myval$classe))
acc1 <- cfmatrix1$overall[1]
cfmatrix1
```

### Model 2: Random Forest

Let's apply the same strategy for a model using a random forest.

``` {r model2}
model2 <- train(classe ~ ., data = mytrain, method = "rf", trControl = control)
pred2 <- predict(model2, myval)
cfmatrix2 <- confusionMatrix(pred2, factor(myval$classe))
acc2 <- cfmatrix2$overall[1]
cfmatrix2
```

### Model 3: Boosting

And finally, let's also perform a model using boosting.

``` {r model3}
model3 <- train(classe ~ ., data = mytrain, method = "gbm", trControl = control, 
                verbose = FALSE)
pred3 <- predict(model3, myval)
cfmatrix3 <- confusionMatrix(pred3, factor(myval$classe))
acc3 <- cfmatrix3$overall[1]
cfmatrix3
```

### Summary

We see very high accuracy rates for the random forest and boosting models at over 99% each.


``` {r summary}
summary <- rbind(acc1, acc2, acc3)
rownames(summary) <- c("Decision Tree", "Random Forest", "Boosting")
summary
```

## Prediction on the Testing Set

Let's choose the random forest model and use it to predict the outcome variable on the testing set.

``` {r pred_test}
testpred <- predict(model2, testing)
testpred
```